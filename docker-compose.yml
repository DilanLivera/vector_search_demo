# to run the compose file: docker compose up -d --force-recreate --build
services:

  ui:
    image: ui
    container_name: ui
    hostname: ui
    build:
      context: .
      dockerfile: src/UI/Dockerfile
    ports:
      - "127.0.0.1:8080:8080"

  aspire-dashboard:
    # https://learn.microsoft.com/samples/dotnet/aspire-samples/aspire-standalone-dashboard
    # https://hub.docker.com/r/microsoft/dotnet-aspire-dashboard/
    image: mcr.microsoft.com/dotnet/aspire-dashboard:latest
    container_name: vector-search-demo-aspire-dashboard
    hostname: aspire-dashboard
    environment:
      - DOTNET_DASHBOARD_UNSECURED_ALLOW_ANONYMOUS=true
    ports:
      - "127.0.0.1:18888:18888"
      - "127.0.0.1:18889:18889"
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: vector-search-demo-aspire-qdrant
    hostname: qdrant
    ports:
      - "127.0.0.1:6333:6333"
      - "127.0.0.1:6334:6334"
    volumes:
      - ./container_volumes/qdrant:/qdrant/storage
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:6333/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s

  # after container is started,
  # run 'ollama pull mxbai-embed-large:335m' command manually in container's terminal
  # or run 'docker exec -it ollama ollama pull mxbai-embed-large:335m' command from a terminal(outside of container)
  # please refer to the https://ollama.com/blog/embedding-models to find a list of embedding models
  # you can use the following request to test
  # curl http://localhost:11434/api/embed -d '{
  #   "model": "mxbai-embed-large",
  #   "input": "Llamas are members of the camelid family"
  # }'
  ollama:
    # https://hub.docker.com/r/ollama/ollama
    image: ollama/ollama:latest
    container_name: ollama
    hostname: ollama
    ports:
      - "127.0.0.1:11434:11434"
    pull_policy: always
    restart: unless-stopped
    tty: true
    volumes:
      - ./container_volumes/ollama:/root/.ollama
